#!/usr/bin/python
"""
Launcher for Tide
Usage: ./tide --help
"""

import os
import sys
import subprocess
import shlex
import distutils.spawn
import argparse
import json
import xml.etree.ElementTree as ET

# rtneuron vglrun env detection
def using_virtualGL():
    return ('VGL_DISPLAY' in os.environ or \
            'VGL_CLIENT' in os.environ) and \
            'SSH_CLIENT' in os.environ

# http://nullege.com/codes/search/distutils.spawn.find_executable
def find_executable(executable, path=None):
    """Tries to find 'executable' in the directories listed in 'path'.

    A string listing directories separated by 'os.pathsep'; defaults to
    os.environ['PATH'].  Returns the complete filename or None if not found.
    """
    if path is None:
        path = os.environ['PATH']
    paths = path.split(os.pathsep)
    base, ext = os.path.splitext(executable)

    if(sys.platform == 'win32' or os.name == 'os2') and (ext != '.exe'):
        executable = executable + '.exe'

    if not os.path.isfile(executable):
        for p in paths:
            f = os.path.join(p, executable)
            if os.path.isfile(f):
                # the file exists, we have a shot at spawn working
                return f
        return None
    else:
        return executable

def locate_binary(exec_name):
    binary = find_executable(exec_name, TIDE_PATH + "/bin/")
    if binary is None:
        binary = find_executable(exec_name)
    if binary is None:
        print exec_name + " executable not found"
        exit(-4)
    return binary

class ProcessConfig:
    def __init__(self, host):
        self.host = host
        self.displays = []

class Configuration:
    def __init__(self):
        self.master = ProcessConfig('localhost')
        self.walls = []

def parse_config_xml(filename):
    config = Configuration()
    xml_config = ET.parse(filename)

    # parse the masterProcess element
    master_process = xml_config.find('masterProcess')
    if master_process is None:
        print("masterProcess not found, using defaults: 'localhost' ':0'")
    else:
        config.master.host = master_process.get("host")
        config.master.displays.append(master_process.get('display'))

    # parse the wall processes
    for process in xml_config.findall('.//process'):
        wall = ProcessConfig(process.get("host"))
        if wall.host is None:
            print('Error, no host attribute in <process> tag.')
            exit(-1)
        display = process.get('display')
        if display is not None: # old format - single display per process
            wall.displays.append(display)
        else: # new format - multiple displays per process
            for screen in process.findall('screen'):
                wall.displays.append(screen.get('display'))
        config.walls.append(wall)
    return config

def parse_config_json(filename):
    config = Configuration()
    data = json.load(open(filename))
    config.master.host = data["master"]["host"]
    config.master.displays.append(data["master"]["display"])
    for process in data["processes"]:
        wall = ProcessConfig(process["host"])
        for screen in process["screens"]:
            wall.displays.append(screen["display"])
        config.walls.append(wall)
    return config

def parse_config(filename):
    extension = os.path.splitext(filename)[1]
    if extension == '.xml':
        return parse_config_xml(filename)
    return parse_config_json(filename)

parser = argparse.ArgumentParser()
parser.add_argument("--config", help="The configuration file to load")
parser.add_argument("--mpiargs", help="Extra arguments for the mpiexec command")
parser.add_argument("--session", help="The session to load")
parser.add_argument("--no-touch", help="Disable TUIO touch plugin",
                    action="store_true")
parser.add_argument("--printcmd", help="Print the command without executing it",
                    action="store_true")
parser.add_argument("--vglrun", help="Run the main application using vglrun (override VirtualGL detection)",
                    action="store_true")
args = parser.parse_args()

# Tide directory; this is the parent directory of this script
TIDE_PATH = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

TIDEMASTER_BIN = locate_binary("tideMaster")
TIDEWALL_BIN = locate_binary("tideWall")
TIDEFORKER_BIN = locate_binary("tideForker")

if args.config:
    TIDE_CONFIG_FILE = os.path.abspath(args.config)
else:
    TIDE_CONFIG_FILE = TIDE_PATH + '/share/Tide/examples/configuration_1x3.json'

# find full path to mpiexec; if MPI is installed in a non-standard location the
# full path may be necessary to launch correctly across the cluster.
MPIRUN_CMD = distutils.spawn.find_executable('mpiexec')

if MPIRUN_CMD is None:
    print('Error, could not find mpiexec executable in PATH')
    exit(-3)

if args.mpiargs:
    mpi_args = args.mpiargs + ' '
else:
    mpi_args = ''

# mpiexec has a different commandline syntax for MVAPICH, MPICH2, and OpenMPI
IS_MVAPICH = distutils.spawn.find_executable('mpiname')
IS_MPICH2 = distutils.spawn.find_executable('mpich2version')
IS_MPICH3 = distutils.spawn.find_executable('mpichversion')
if IS_MVAPICH:
    MPI_SPECIAL_FLAGS = '-env MV2_ENABLE_AFFINITY 0 -env IPATH_NO_CPUAFFINITY 1'
    MPI_GLOBAL_HOST_LIST = '-hosts'
    MPI_PER_NODE_HOST = None
    EXPORT_ENV_VAR = '-env {0} {1}'
    mpi_args += '-genvlist MPIEXEC_SIGNAL_PROPAGATION,LD_LIBRARY_PATH'
elif IS_MPICH2:
    MPI_SPECIAL_FLAGS = '-env MV2_ENABLE_AFFINITY 0 -env IPATH_NO_CPUAFFINITY 1'
    MPI_GLOBAL_HOST_LIST = None
    MPI_PER_NODE_HOST = '-host'
    EXPORT_ENV_VAR = '-env {0} {1}'
    mpi_args += '-genvlist LD_LIBRARY_PATH'
elif IS_MPICH3:
    MPI_SPECIAL_FLAGS = '-env MV2_ENABLE_AFFINITY 0 -env IPATH_NO_CPUAFFINITY 1'
    MPI_GLOBAL_HOST_LIST = '-hosts'
    MPI_PER_NODE_HOST = None
    EXPORT_ENV_VAR = '-env {0} {1}'
    mpi_args += '-genvlist LD_LIBRARY_PATH'
else: # OpenMPI
    # oversubscribe is needed with OpenMPI 3.x otherwise the process fails with:
    # "All nodes which are allocated for this job are already filled."
    # It does not cause problems with older versions (1.10).
    MPI_SPECIAL_FLAGS = '-x IPATH_NO_CPUAFFINITY=1 --oversubscribe'
    if os.getenv("LD_LIBRARY_PATH"): # set and not empty
        MPI_SPECIAL_FLAGS += ' -x LD_LIBRARY_PATH'
    MPI_GLOBAL_HOST_LIST = '-host'
    MPI_PER_NODE_HOST = '-H'
    EXPORT_ENV_VAR = '-x {0}={1}'
    # "hwloc_base_binding_policy none" is equivalent to the new "--bind-to none"
    # option in OpenMPI >= 1.8, while being accepted by older versions as well.
    # It does nothing in that case, but the (now deprecated) "--bind-to-none"
    # was the default anyways.
    mpi_args += '--mca hwloc_base_binding_policy none'

# Form the application parameters list
TIDE_PARAMS = '--config ' + TIDE_CONFIG_FILE
# Note that the "session" arg is reserved by Qt so "sessionfile" is used instead
if args.session:
    TIDE_PARAMS += ' --sessionfile ' + args.session

# form the MPI host list
hostlist = []

# Form the list of commands to execute
runcommands = []

# Execute master process with vglrun
if args.vglrun:
    VGLRUN_BIN = 'vglrun '
else:
    VGLRUN_BIN = ''

# Override VirtualGL detection
using_vgl = not args.vglrun and using_virtualGL()

# configuration file gives the list of the hosts and the displays
try:
    config = parse_config(TIDE_CONFIG_FILE)
    if using_vgl:
        config.master.displays[0] = os.environ["DISPLAY"]
        for wall in config.walls:
            wall.displays[0] = os.environ["DISPLAY"]

    # add the master and forker commands
    if MPI_PER_NODE_HOST:
        node_host = '%s %s' % (MPI_PER_NODE_HOST, config.master.host)
    else:
        node_host = ''

    if MPI_GLOBAL_HOST_LIST:
        hostlist.append(config.master.host) # master
        forker_host = config.master.host    # forker goes at the end of hostlist

    export_display = EXPORT_ENV_VAR.format('DISPLAY', config.master.displays[0])
    env_vars = [MPI_SPECIAL_FLAGS, export_display, node_host]
    environment = ' '.join(env_vars)

    # add a separate 'forker' process on the same host as the master process
    forkercmd = '%s -np 1 %s' % (environment, TIDEFORKER_BIN)

    if not args.no_touch:
        tuio = EXPORT_ENV_VAR.format('QT_QPA_GENERIC_PLUGINS', 'TuioTouch')
        env_vars += [tuio]
        environment = ' '.join(env_vars)
    masterexec = '%s%s %s' % (VGLRUN_BIN, TIDEMASTER_BIN, TIDE_PARAMS)
    mastercmd = '%s -np 1 %s' % (environment, masterexec)
    runcommands.append(mastercmd)

    # add the wall commands
    for wall in config.walls:
        export_display = EXPORT_ENV_VAR.format('DISPLAY', wall.displays[0])
        if len(wall.displays) > 1: # multiple screens per process, needs QPA
            xcb_config = ['xcb'] + wall.displays
            qpa = ':'.join(xcb_config)
            export_qpa = EXPORT_ENV_VAR.format('QT_QPA_PLATFORM', qpa)
            export_display = '%s %s' % (export_display, export_qpa)

        if MPI_PER_NODE_HOST:
            node_host = '%s %s' % (MPI_PER_NODE_HOST, wall.host)
        else:
            node_host = ''

        if MPI_GLOBAL_HOST_LIST:
            hostlist.append(wall.host)

        env_vars = [MPI_SPECIAL_FLAGS, export_display, node_host]
        environment = ' '.join(env_vars)
        wallcmd = '%s -np 1 %s' % (environment, TIDEWALL_BIN)
        runcommands.append(wallcmd)

    # the 'forker' process is the last one
    if MPI_GLOBAL_HOST_LIST:
        hostlist.append(forker_host)
    runcommands.append(forkercmd)

except Exception as e:
    print("Error processing configuration '%s'. (%s)" % (TIDE_CONFIG_FILE, e))
    exit(-2)

if MPI_GLOBAL_HOST_LIST:
    HOST_LIST = '%s %s' % (MPI_GLOBAL_HOST_LIST, ",".join(hostlist))
else:
    HOST_LIST = ''

RUN_COMMANDS = ' : '.join(runcommands)
START_CMD = '%s %s %s %s' % (MPIRUN_CMD, mpi_args, HOST_LIST, RUN_COMMANDS)

if args.printcmd:
    print(START_CMD)
else:
    exit(subprocess.call(shlex.split(START_CMD)))
